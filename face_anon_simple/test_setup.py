#!/usr/bin/env python3
"""
æµ‹è¯•face_anon_simpleé¡¹ç›®è®¾ç½®
"""

import torch
from transformers import CLIPImageProcessor, CLIPVisionModel
from diffusers import AutoencoderKL, DDPMScheduler
from diffusers.utils import load_image
from src.diffusers.models.referencenet.referencenet_unet_2d_condition import (
    ReferenceNetModel,
)
from src.diffusers.models.referencenet.unet_2d_condition import UNet2DConditionModel
from src.diffusers.pipelines.referencenet.pipeline_referencenet import (
    StableDiffusionReferenceNetPipeline,
)

def test_setup():
    print("ğŸš€ å¼€å§‹æµ‹è¯•face_anon_simpleé¡¹ç›®è®¾ç½®...")
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    print(f"âœ… CUDAå¯ç”¨: {torch.cuda.is_available()}")
    print(f"âœ… GPUæ•°é‡: {torch.cuda.device_count()}")
    
    try:
        print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
        face_model_id = "hkung/face-anon-simple"
        clip_model_id = "openai/clip-vit-large-patch14"
        sd_model_id = "stabilityai/stable-diffusion-2-1"

        print("  - åŠ è½½UNetæ¨¡å‹...")
        unet = UNet2DConditionModel.from_pretrained(
            face_model_id, subfolder="unet", use_safetensors=True
        )
        
        print("  - åŠ è½½ReferenceNetæ¨¡å‹...")
        referencenet = ReferenceNetModel.from_pretrained(
            face_model_id, subfolder="referencenet", use_safetensors=True
        )
        
        conditioning_referencenet = ReferenceNetModel.from_pretrained(
            face_model_id, subfolder="conditioning_referencenet", use_safetensors=True
        )
        
        print("  - åŠ è½½VAEæ¨¡å‹...")
        vae = AutoencoderKL.from_pretrained(sd_model_id, subfolder="vae", use_safetensors=True)
        
        print("  - åŠ è½½è°ƒåº¦å™¨...")
        scheduler = DDPMScheduler.from_pretrained(
            sd_model_id, subfolder="scheduler", use_safetensors=True
        )
        
        print("  - åŠ è½½CLIPæ¨¡å‹...")
        feature_extractor = CLIPImageProcessor.from_pretrained(
            clip_model_id, use_safetensors=True
        )
        image_encoder = CLIPVisionModel.from_pretrained(clip_model_id, use_safetensors=True)

        print("ğŸ”§ åˆ›å»ºå¤„ç†ç®¡é“...")
        pipe = StableDiffusionReferenceNetPipeline(
            unet=unet,
            referencenet=referencenet,
            conditioning_referencenet=conditioning_referencenet,
            vae=vae,
            feature_extractor=feature_extractor,
            image_encoder=image_encoder,
            scheduler=scheduler,
        )
        
        print("ğŸšš å°†æ¨¡å‹ç§»åŠ¨åˆ°GPU...")
        pipe = pipe.to("cuda")
        
        print("ğŸ–¼ï¸ æµ‹è¯•å›¾åƒåŠ è½½...")
        original_image = load_image("my_dataset/test/14795.png")
        print(f"  - å›¾åƒå°ºå¯¸: {original_image.size}")
        
        print("âœ… æ‰€æœ‰ç»„ä»¶åŠ è½½æˆåŠŸï¼")
        print("ğŸ‰ face_anon_simpleé¡¹ç›®è®¾ç½®å®Œæˆï¼Œå¯ä»¥æ­£å¸¸ä½¿ç”¨!")
        
        return True
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
        return False

if __name__ == "__main__":
    success = test_setup()
    if success:
        print("\nğŸŠ é¡¹ç›®è®¾ç½®æµ‹è¯•æˆåŠŸ!")
    else:
        print("\nğŸ’¥ é¡¹ç›®è®¾ç½®æµ‹è¯•å¤±è´¥!")
