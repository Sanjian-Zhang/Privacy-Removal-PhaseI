#!/usr/bin/env python3
"""
Face Anonymization Made Simple - å•å¼ å›¾ç‰‡å¤„ç†è„šæœ¬ (ä½¿ç”¨GPU 1)
"""

import os
import torch
import gc
from transformers import CLIPImageProcessor, CLIPVisionModel
from diffusers import AutoencoderKL, DDPMScheduler
from diffusers.utils import load_image
from src.diffusers.models.referencenet.referencenet_unet_2d_condition import (
    ReferenceNetModel,
)
from src.diffusers.models.referencenet.unet_2d_condition import UNet2DConditionModel
from src.diffusers.pipelines.referencenet.pipeline_referencenet import (
    StableDiffusionReferenceNetPipeline,
)

def clear_cuda_memory():
    """æ¸…ç† CUDA å†…å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()

def setup_pipeline():
    """è®¾ç½®äººè„¸åŒ¿ååŒ–å¤„ç†ç®¡é“ - ä½¿ç”¨GPU 1"""
    print("ğŸ”§ è®¾ç½®å¤„ç†ç®¡é“ (ä½¿ç”¨ GPU 1)...")
    
    # è®¾ç½®ä½¿ç”¨ GPU 1
    device = "cuda:1"
    
    # æ¸…ç†å†…å­˜
    clear_cuda_memory()
    
    face_model_id = "hkung/face-anon-simple"
    clip_model_id = "openai/clip-vit-large-patch14"
    sd_model_id = "stabilityai/stable-diffusion-2-1"

    # åŠ è½½æ¨¡å‹
    print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
    unet = UNet2DConditionModel.from_pretrained(
        face_model_id, subfolder="unet", use_safetensors=True, torch_dtype=torch.float16
    )
    referencenet = ReferenceNetModel.from_pretrained(
        face_model_id, subfolder="referencenet", use_safetensors=True, torch_dtype=torch.float16
    )
    conditioning_referencenet = ReferenceNetModel.from_pretrained(
        face_model_id, subfolder="conditioning_referencenet", use_safetensors=True, torch_dtype=torch.float16
    )
    vae = AutoencoderKL.from_pretrained(sd_model_id, subfolder="vae", use_safetensors=True, torch_dtype=torch.float16)
    scheduler = DDPMScheduler.from_pretrained(
        sd_model_id, subfolder="scheduler", use_safetensors=True
    )
    feature_extractor = CLIPImageProcessor.from_pretrained(
        clip_model_id, use_safetensors=True
    )
    image_encoder = CLIPVisionModel.from_pretrained(clip_model_id, use_safetensors=True, torch_dtype=torch.float16)

    # åˆ›å»ºç®¡é“
    print("ğŸ”— åˆ›å»ºå¤„ç†ç®¡é“...")
    pipe = StableDiffusionReferenceNetPipeline(
        unet=unet,
        referencenet=referencenet,
        conditioning_referencenet=conditioning_referencenet,
        vae=vae,
        feature_extractor=feature_extractor,
        image_encoder=image_encoder,
        scheduler=scheduler,
    )
    
    # å¯ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›
    if hasattr(pipe, 'enable_xformers_memory_efficient_attention'):
        pipe.enable_xformers_memory_efficient_attention()
    
    # å¯ç”¨å†…å­˜é«˜æ•ˆVAE
    if hasattr(pipe.vae, 'enable_slicing'):
        pipe.vae.enable_slicing()
    if hasattr(pipe.vae, 'enable_tiling'):
        pipe.vae.enable_tiling()
    
    pipe = pipe.to(device, torch_dtype=torch.float16)
    
    return pipe

def anonymize_single_image(input_path, output_path, anonymization_degree=1.25):
    """å¤„ç†å•å¼ å›¾åƒ"""
    try:
        print(f"ğŸ“¸ å¤„ç†å›¾åƒ: {input_path}")
        
        # è®¾ç½®ç®¡é“
        pipe = setup_pipeline()
        
        # åŠ è½½å›¾åƒ
        original_image = load_image(input_path)
        print(f"ğŸ–¼ï¸ å›¾åƒå°ºå¯¸: {original_image.size}")
        
        # ä½¿ç”¨æ›´å°çš„å°ºå¯¸ä»¥èŠ‚çœå†…å­˜
        target_size = 256  # è¿›ä¸€æ­¥å‡å°å°ºå¯¸
        width, height = original_image.size
        if width > target_size or height > target_size:
            if width > height:
                new_width = target_size
                new_height = int(height * target_size / width)
            else:
                new_height = target_size
                new_width = int(width * target_size / height)
        else:
            new_width, new_height = width, height
        
        print(f"ğŸ”„ è°ƒæ•´åå°ºå¯¸: {new_width}x{new_height}")
        
        # ç”ŸæˆåŒ¿ååŒ–å›¾åƒ
        generator = torch.manual_seed(42)
        
        # ä½¿ç”¨æ›´å°‘çš„æ¨ç†æ­¥æ•°
        with torch.cuda.amp.autocast():
            anon_image = pipe(
                source_image=original_image,
                conditioning_image=original_image,
                num_inference_steps=20,  # å¤§å¹…å‡å°‘æ­¥æ•°
                guidance_scale=4.0,
                generator=generator,
                anonymization_degree=anonymization_degree,
                width=new_width,
                height=new_height,
            ).images[0]
        
        # ä¿å­˜ç»“æœ
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        anon_image.save(output_path)
        
        print(f"âœ… å¤„ç†å®Œæˆ: {output_path}")
        
        # æ¸…ç†å†…å­˜
        del pipe, anon_image, original_image
        clear_cuda_memory()
        
        return True
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {str(e)}")
        clear_cuda_memory()
        return False

def main():
    # å¤„ç†ç¬¬ä¸€å¼ å›¾ç‰‡
    input_path = "/home/zhiqics/sanjian/dataset/test_images/images/frame_00032.jpg"
    output_path = "/home/zhiqics/sanjian/dataset/test_images/anon/frame_00032_anon.jpg"
    
    print("ğŸ­ Face Anonymization Made Simple - å•å›¾å¤„ç† (GPU 1)")
    print("=" * 50)
    print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_path}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")
    print("=" * 50)
    
    if not os.path.exists(input_path):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return
    
    success = anonymize_single_image(input_path, output_path)
    
    if success:
        print("ğŸ‰ å¤„ç†æˆåŠŸå®Œæˆ!")
    else:
        print("âŒ å¤„ç†å¤±è´¥!")

if __name__ == "__main__":
    main()
