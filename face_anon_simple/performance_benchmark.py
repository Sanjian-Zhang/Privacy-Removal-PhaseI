#!/usr/bin/env python3
"""
Face Anonymization Made Simple - æ€§èƒ½åŸºå‡†æµ‹è¯•
ç”¨äºæµ‹è¯•ä¸åŒé…ç½®ä¸‹çš„å¤„ç†é€Ÿåº¦
"""

import torch
import time
import os
from PIL import Image
import numpy as np

def check_gpu_setup():
    """æ£€æŸ¥GPUè®¾ç½®"""
    print("ğŸ” GPUç¯å¢ƒæ£€æŸ¥:")
    print(f"  - CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"  - GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"  - å½“å‰GPU: {torch.cuda.current_device()}")
        print(f"  - GPUåç§°: {torch.cuda.get_device_name()}")
        
        # æ˜¾å­˜ä¿¡æ¯
        total_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
        allocated_mem = torch.cuda.memory_allocated() / 1024**3
        reserved_mem = torch.cuda.memory_reserved() / 1024**3
        
        print(f"  - æ€»æ˜¾å­˜: {total_mem:.1f}GB")
        print(f"  - å·²åˆ†é…: {allocated_mem:.1f}GB")
        print(f"  - å·²ä¿ç•™: {reserved_mem:.1f}GB")
        print(f"  - å¯ç”¨æ˜¾å­˜: {total_mem - reserved_mem:.1f}GB")
        
        # CUDAç‰ˆæœ¬
        print(f"  - CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"  - PyTorchç‰ˆæœ¬: {torch.__version__}")
        
        return True
    else:
        print("  âŒ GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUå¤„ç†ï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰")
        return False

def test_model_loading_speed():
    """æµ‹è¯•æ¨¡å‹åŠ è½½é€Ÿåº¦"""
    print("\nğŸš€ æµ‹è¯•æ¨¡å‹åŠ è½½é€Ÿåº¦...")
    
    try:
        from transformers import CLIPImageProcessor, CLIPVisionModel
        from diffusers import AutoencoderKL, DDPMScheduler
        from src.diffusers.models.referencenet.referencenet_unet_2d_condition import ReferenceNetModel
        from src.diffusers.models.referencenet.unet_2d_condition import UNet2DConditionModel
        from src.diffusers.pipelines.referencenet.pipeline_referencenet import StableDiffusionReferenceNetPipeline
        
        face_model_id = "hkung/face-anon-simple"
        clip_model_id = "openai/clip-vit-large-patch14"
        sd_model_id = "stabilityai/stable-diffusion-2-1"
        
        start_time = time.time()
        
        # æµ‹è¯•ä¸åŒç²¾åº¦çš„åŠ è½½æ—¶é—´
        for precision, torch_dtype in [("float32", torch.float32), ("float16", torch.float16)]:
            print(f"\n  ğŸ“¦ æµ‹è¯• {precision} ç²¾åº¦åŠ è½½:")
            precision_start = time.time()
            
            try:
                unet = UNet2DConditionModel.from_pretrained(
                    face_model_id, subfolder="unet", use_safetensors=True, torch_dtype=torch_dtype
                )
                unet_time = time.time() - precision_start
                print(f"    - UNetåŠ è½½æ—¶é—´: {unet_time:.2f}ç§’")
                
                referencenet = ReferenceNetModel.from_pretrained(
                    face_model_id, subfolder="referencenet", use_safetensors=True, torch_dtype=torch_dtype
                )
                ref_time = time.time() - precision_start - unet_time
                print(f"    - ReferenceNetåŠ è½½æ—¶é—´: {ref_time:.2f}ç§’")
                
                del unet, referencenet
                torch.cuda.empty_cache()
                
            except Exception as e:
                print(f"    âŒ {precision} ç²¾åº¦åŠ è½½å¤±è´¥: {e}")
        
    except ImportError as e:
        print(f"  âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    return True

def benchmark_inference_steps():
    """åŸºå‡†æµ‹è¯•ä¸åŒæ¨ç†æ­¥æ•°çš„é€Ÿåº¦"""
    print("\nâš¡ æ¨ç†æ­¥æ•°æ€§èƒ½å¯¹æ¯”:")
    
    steps_to_test = [10, 25, 50, 100, 200]
    
    print("æ¨ç†æ­¥æ•° | é¢„ä¼°å•å›¾å¤„ç†æ—¶é—´ | é€Ÿåº¦æå‡å€æ•°")
    print("-" * 45)
    
    base_time_per_step = 0.15  # ä¼°ç®—æ¯æ­¥éœ€è¦0.15ç§’ï¼ˆåŸºäºGPUæ€§èƒ½ï¼‰
    
    for steps in steps_to_test:
        estimated_time = steps * base_time_per_step
        speedup = (200 * base_time_per_step) / estimated_time
        print(f"{steps:^8} | {estimated_time:^15.1f}ç§’ | {speedup:^11.1f}x")

def create_test_image(size=(512, 512)):
    """åˆ›å»ºæµ‹è¯•å›¾åƒ"""
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾åƒ
    img_array = np.random.randint(0, 255, (size[1], size[0], 3), dtype=np.uint8)
    return Image.fromarray(img_array)

def memory_usage_test():
    """æ˜¾å­˜ä½¿ç”¨æµ‹è¯•"""
    print("\nğŸ’¾ æ˜¾å­˜ä½¿ç”¨æƒ…å†µæµ‹è¯•:")
    
    if not torch.cuda.is_available():
        print("  âŒ GPUä¸å¯ç”¨ï¼Œè·³è¿‡æ˜¾å­˜æµ‹è¯•")
        return
    
    torch.cuda.empty_cache()
    initial_mem = torch.cuda.memory_allocated() / 1024**2
    print(f"  - åˆå§‹æ˜¾å­˜ä½¿ç”¨: {initial_mem:.1f}MB")
    
    # æ¨¡æ‹Ÿä¸åŒå°ºå¯¸å›¾åƒçš„æ˜¾å­˜ä½¿ç”¨
    sizes = [(256, 256), (512, 512), (768, 768), (1024, 1024)]
    
    for size in sizes:
        # åˆ›å»ºæ¨¡æ‹Ÿå¼ é‡æ¥ä¼°ç®—æ˜¾å­˜ä½¿ç”¨
        try:
            # æ¨¡æ‹Ÿè¾“å…¥å›¾åƒtensor
            img_tensor = torch.randn(1, 3, size[1], size[0], device='cuda', dtype=torch.float16)
            current_mem = torch.cuda.memory_allocated() / 1024**2
            mem_used = current_mem - initial_mem
            print(f"  - {size[0]}x{size[1]} å›¾åƒçº¦éœ€æ˜¾å­˜: {mem_used:.1f}MB")
            del img_tensor
            torch.cuda.empty_cache()
        except RuntimeError as e:
            print(f"  - {size[0]}x{size[1]} å›¾åƒæ˜¾å­˜ä¸è¶³: {e}")

def optimization_recommendations():
    """æ€§èƒ½ä¼˜åŒ–å»ºè®®"""
    print("\nğŸ¯ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
    
    gpu_available = torch.cuda.is_available()
    total_mem = 0
    
    if gpu_available:
        total_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
    
    print("\n1. æ¨ç†æ­¥æ•°ä¼˜åŒ–:")
    print("   - é»˜è®¤200æ­¥ â†’ å»ºè®®25-50æ­¥ (4-8xé€Ÿåº¦æå‡)")
    print("   - è´¨é‡ç¨æœ‰ä¸‹é™ï¼Œä½†é€Ÿåº¦å¤§å¹…æå‡")
    print("   - å»ºè®®æµ‹è¯•èŒƒå›´: 25, 50, 100æ­¥")
    
    print("\n2. ç²¾åº¦ä¼˜åŒ–:")
    if gpu_available and total_mem >= 6:
        print("   - ä½¿ç”¨float16åŠç²¾åº¦ (æ¨è)")
        print("   - æ˜¾å­˜ä½¿ç”¨å‡åŠï¼Œé€Ÿåº¦æå‡20-30%")
    else:
        print("   - æ˜¾å­˜è¾ƒå°ï¼Œå¼ºçƒˆå»ºè®®ä½¿ç”¨float16")
    
    print("\n3. å†…å­˜ä¼˜åŒ–:")
    print("   - å¯ç”¨attention_slicing")
    print("   - å¯ç”¨xformers (å¦‚æœå¯ç”¨)")
    print("   - å¯ç”¨model_cpu_offload (æ˜¾å­˜ä¸è¶³æ—¶)")
    
    print("\n4. æ‰¹å¤„ç†ä¼˜åŒ–:")
    print("   - å›¾åƒå°ºå¯¸æ ‡å‡†åŒ– (512x512)")
    print("   - è·³è¿‡å·²å¤„ç†çš„å›¾åƒ")
    print("   - é¢„åŠ è½½æ¨¡å‹ï¼Œé¿å…é‡å¤åŠ è½½")
    
    print("\n5. ç¡¬ä»¶å»ºè®®:")
    if gpu_available:
        print(f"   - å½“å‰GPU: {torch.cuda.get_device_name()}")
        if total_mem < 6:
            print("   - æ˜¾å­˜åå°ï¼Œå»ºè®®ä½¿ç”¨cloud GPU")
        elif total_mem >= 12:
            print("   - æ˜¾å­˜å……è¶³ï¼Œå¯ä»¥å¤„ç†å¤§å›¾æˆ–æ‰¹é‡å¤„ç†")
    else:
        print("   - å¼ºçƒˆå»ºè®®ä½¿ç”¨GPUï¼ŒCPUå¤„ç†ä¼šéå¸¸æ…¢")

def main():
    print("ğŸ­ Face Anonymization Made Simple - æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 60)
    
    # GPUç¯å¢ƒæ£€æŸ¥
    gpu_ok = check_gpu_setup()
    
    # æ¨¡å‹åŠ è½½é€Ÿåº¦æµ‹è¯•
    if gpu_ok:
        test_model_loading_speed()
    
    # æ¨ç†æ­¥æ•°åŸºå‡†æµ‹è¯•
    benchmark_inference_steps()
    
    # æ˜¾å­˜ä½¿ç”¨æµ‹è¯•
    memory_usage_test()
    
    # ä¼˜åŒ–å»ºè®®
    optimization_recommendations()
    
    print("\nğŸ¯ å¿«é€Ÿä¼˜åŒ–å‘½ä»¤:")
    print("ä½¿ç”¨ä¼˜åŒ–ç‰ˆæ‰¹å¤„ç†è„šæœ¬:")
    print("python batch_anonymize_optimized.py \\")
    print("    --input_dir /path/to/input \\")
    print("    --output_dir /path/to/output \\")
    print("    --num_inference_steps 25 \\")
    print("    --use_fp16")
    
    print("\nğŸ“ æ€§èƒ½å¯¹æ¯” (é¢„ä¼°):")
    print("- åŸå§‹è®¾ç½®: ~30ç§’/å›¾åƒ (200æ­¥, float32)")
    print("- ä¼˜åŒ–è®¾ç½®: ~4ç§’/å›¾åƒ (25æ­¥, float16)")
    print("- é€Ÿåº¦æå‡: ~8å€")

if __name__ == "__main__":
    main()
