#!/usr/bin/env python3
"""
Face Anonymization Made Simple - ä¼˜åŒ–ç‰ˆæ‰¹é‡åŒ¿ååŒ–å¤„ç†è„šæœ¬
é€šè¿‡å‡å°‘æ¨ç†æ­¥æ•°ã€ä½¿ç”¨åŠç²¾åº¦ç­‰æ–¹å¼æå‡å¤„ç†é€Ÿåº¦ï¼Œå¹¶æ™ºèƒ½å¤„ç†å¤§å°ºå¯¸å›¾ç‰‡
"""

import os
import argparse
import glob
import torch
from tqdm import tqdm
from pathlib import Path
from PIL import Image
import time

try:
    from transformers import CLIPImageProcessor, CLIPVisionModel
    from diffusers import AutoencoderKL, DDPMScheduler
    from diffusers.utils import load_image
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿å·²å®‰è£…æ‰€éœ€ä¾èµ–: pip install transformers diffusers")
from src.diffusers.models.referencenet.referencenet_unet_2d_condition import (
    ReferenceNetModel,
)
from src.diffusers.models.referencenet.unet_2d_condition import UNet2DConditionModel
from src.diffusers.pipelines.referencenet.pipeline_referencenet import (
    StableDiffusionReferenceNetPipeline,
)

def setup_pipeline(use_fp16=True, enable_attention_slicing=True):
    """è®¾ç½®ä¼˜åŒ–çš„äººè„¸åŒ¿ååŒ–å¤„ç†ç®¡é“"""
    print("ğŸ”§ è®¾ç½®ä¼˜åŒ–å¤„ç†ç®¡é“...")
    
    face_model_id = "hkung/face-anon-simple"
    clip_model_id = "openai/clip-vit-large-patch14"
    sd_model_id = "stabilityai/stable-diffusion-2-1"

    # åŠ è½½æ¨¡å‹æ—¶ä½¿ç”¨åŠç²¾åº¦
    torch_dtype = torch.float16 if use_fp16 else torch.float32
    
    print("  - åŠ è½½UNetæ¨¡å‹...")
    unet = UNet2DConditionModel.from_pretrained(
        face_model_id, subfolder="unet", use_safetensors=True, torch_dtype=torch_dtype
    )
    
    print("  - åŠ è½½ReferenceNetæ¨¡å‹...")
    referencenet = ReferenceNetModel.from_pretrained(
        face_model_id, subfolder="referencenet", use_safetensors=True, torch_dtype=torch_dtype
    )
    
    print("  - åŠ è½½Conditioning ReferenceNetæ¨¡å‹...")
    conditioning_referencenet = ReferenceNetModel.from_pretrained(
        face_model_id, subfolder="conditioning_referencenet", use_safetensors=True, torch_dtype=torch_dtype
    )
    
    print("  - åŠ è½½VAE...")
    vae = AutoencoderKL.from_pretrained(
        sd_model_id, subfolder="vae", use_safetensors=True, torch_dtype=torch_dtype
    )
    
    print("  - åŠ è½½Scheduler...")
    scheduler = DDPMScheduler.from_pretrained(
        sd_model_id, subfolder="scheduler", use_safetensors=True
    )
    
    print("  - åŠ è½½CLIPç‰¹å¾æå–å™¨...")
    feature_extractor = CLIPImageProcessor.from_pretrained(clip_model_id)
    
    print("  - åŠ è½½CLIPå›¾åƒç¼–ç å™¨...")
    image_encoder = CLIPVisionModel.from_pretrained(
        clip_model_id, use_safetensors=True, torch_dtype=torch_dtype
    )

    # åˆ›å»ºç®¡é“
    pipe = StableDiffusionReferenceNetPipeline(
        unet=unet,
        referencenet=referencenet,
        conditioning_referencenet=conditioning_referencenet,
        vae=vae,
        feature_extractor=feature_extractor,
        image_encoder=image_encoder,
        scheduler=scheduler,
    )
    
    # ç§»åŠ¨åˆ°GPU
    pipe = pipe.to("cuda")
    
    # å¯ç”¨å†…å­˜ä¼˜åŒ–
    if enable_attention_slicing:
        pipe.enable_attention_slicing()
    
    # å¯ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    try:
        pipe.enable_xformers_memory_efficient_attention()
        print("  âœ… å¯ç”¨xformerså†…å­˜é«˜æ•ˆæ³¨æ„åŠ›")
    except:
        print("  âš ï¸ xformersä¸å¯ç”¨ï¼Œè·³è¿‡å†…å­˜ä¼˜åŒ–")
    
    # å¯ç”¨æ¨¡å‹å¸è½½ä»¥èŠ‚çœæ˜¾å­˜
    try:
        pipe.enable_model_cpu_offload()
        print("  âœ… å¯ç”¨æ¨¡å‹CPUå¸è½½")
    except:
        print("  âš ï¸ æ— æ³•å¯ç”¨æ¨¡å‹CPUå¸è½½")
    
    print("âœ… ä¼˜åŒ–ç®¡é“è®¾ç½®å®Œæˆ")
    return pipe

def preprocess_image(image_path, max_pixels=1920*1080):
    """é¢„å¤„ç†å›¾åƒï¼Œå¤„ç†å¤§å°ºå¯¸å›¾ç‰‡ä»¥é¿å…å†…å­˜é—®é¢˜"""
    try:
        # åŠ è½½å›¾åƒ
        original_image = load_image(image_path)
        
        # ç¡®ä¿å›¾åƒæ¨¡å¼æ­£ç¡®
        if original_image.mode != 'RGB':
            original_image = original_image.convert('RGB')
        
        # æ™ºèƒ½å°ºå¯¸å¤„ç†ï¼šä¿æŒåŸå§‹å°ºå¯¸ï¼Œä½†å¯¹è¶…å¤§å›¾ç‰‡è¿›è¡Œç¼©æ”¾é¿å…æ˜¾å­˜ä¸è¶³
        w, h = original_image.size
        original_w, original_h = w, h
        
        # è®¡ç®—åƒç´ æ•°é‡ï¼Œå¦‚æœè¶…è¿‡é˜ˆå€¼åˆ™ç¼©æ”¾
        current_pixels = w * h
        
        if current_pixels > max_pixels:
            # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
            scale_factor = (max_pixels / current_pixels) ** 0.5
            new_w = int(w * scale_factor)
            new_h = int(h * scale_factor)
            print(f"  å›¾ç‰‡è¿‡å¤§ï¼Œä¸´æ—¶ç¼©æ”¾: {w}x{h} â†’ {new_w}x{new_h} (ç¼©æ”¾æ¯”ä¾‹: {scale_factor:.3f})")
        else:
            new_w, new_h = w, h
        
        # ç¡®ä¿å°ºå¯¸æ˜¯8çš„å€æ•°ï¼ˆæ‰©æ•£æ¨¡å‹è¦æ±‚ï¼‰
        new_w = (new_w // 8) * 8
        new_h = (new_h // 8) * 8
        
        # å¦‚æœè°ƒæ•´åçš„å°ºå¯¸ä¸º0ï¼Œè®¾ç½®æœ€å°å€¼
        if new_w == 0:
            new_w = 8
        if new_h == 0:
            new_h = 8
            
        return original_image, (original_w, original_h), (new_w, new_h)
    except Exception as e:
        print(f"âŒ é¢„å¤„ç†å›¾åƒå¤±è´¥: {str(e)}")
        return None, None, None

def anonymize_image_fast(pipe, image_path, output_path, 
                        anonymization_degree=1.25, 
                        num_inference_steps=25,
                        guidance_scale=4.0):
    """å¿«é€ŸåŒ¿ååŒ–å•ä¸ªå›¾åƒ"""
    try:
        # é¢„å¤„ç†å›¾åƒ
        original_image, original_size, processing_size = preprocess_image(image_path)
        if original_image is None:
            return False, 0
            
        original_w, original_h = original_size
        new_w, new_h = processing_size
        
        print(f"  å¤„ç†å°ºå¯¸: {new_w}x{new_h}")
        
        # ç”ŸæˆåŒ¿ååŒ–å›¾åƒ
        generator = torch.manual_seed(42)
        
        start_time = time.time()
        
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        anon_image = pipe(
            source_image=original_image,
            conditioning_image=original_image,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
            generator=generator,
            anonymization_degree=anonymization_degree,
            width=new_w,
            height=new_h,
        ).images[0]
        
        processing_time = time.time() - start_time
        
        # è°ƒæ•´å›åŸå§‹å°ºå¯¸ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if (new_w, new_h) != (original_w, original_h):
            print(f"  è°ƒæ•´å›åŸå§‹å°ºå¯¸: {original_w}x{original_h}")
            anon_image = anon_image.resize((original_w, original_h), Image.Resampling.LANCZOS)
        
        # ä¿å­˜ç»“æœ - ä½¿ç”¨é«˜è´¨é‡è®¾ç½®ä¿æŒåŸå§‹è´¨é‡
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # å¦‚æœåŸå›¾æ˜¯PNGï¼Œä¿å­˜ä¸ºPNGä¿æŒæ— æŸï¼›å¦åˆ™ä½¿ç”¨é«˜è´¨é‡JPEG
        if image_path.lower().endswith('.png'):
            anon_image.save(output_path.replace('.jpg', '.png'), format='PNG')
        else:
            anon_image.save(output_path, format='JPEG', quality=98, optimize=False)
        
        return True, processing_time
    except Exception as e:
        print(f"âŒ å¤„ç† {image_path} å¤±è´¥: {str(e)}")
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        return False, 0

def process_batch_optimized(args):
    """ä¼˜åŒ–ç‰ˆæ‰¹é‡å¤„ç†å›¾åƒ"""
    # è·å–æ‰€æœ‰è¾“å…¥æ–‡ä»¶
    input_pattern = os.path.join(args.input_dir, args.pattern)
    image_files = glob.glob(input_pattern, recursive=True)
    
    if not image_files:
        print(f"âŒ åœ¨ {input_pattern} ä¸­æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(image_files)} ä¸ªå›¾åƒæ–‡ä»¶")
    
    # è®¾ç½®ç®¡é“
    pipe = setup_pipeline(use_fp16=args.use_fp16)
    
    # ç»Ÿè®¡ä¿¡æ¯
    success_count = 0
    total_time = 0
    
    # å¤„ç†æ¯ä¸ªå›¾åƒ
    for image_file in tqdm(image_files, desc="å¤„ç†å›¾åƒ"):
        # ç”Ÿæˆè¾“å‡ºè·¯å¾„
        rel_path = os.path.relpath(image_file, args.input_dir)
        output_path = os.path.join(args.output_dir, rel_path)
        
        # æ£€æŸ¥æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶
        if os.path.exists(output_path) and not args.overwrite:
            print(f"â­ï¸ è·³è¿‡å·²å­˜åœ¨æ–‡ä»¶: {output_path}")
            continue
        
        # å¤„ç†å›¾åƒ
        success, proc_time = anonymize_image_fast(
            pipe, image_file, output_path,
            anonymization_degree=args.anonymization_degree,
            num_inference_steps=args.num_inference_steps,
            guidance_scale=args.guidance_scale
        )
        
        if success:
            success_count += 1
            total_time += proc_time
            avg_time = total_time / success_count
            print(f"âœ… å¤„ç†å®Œæˆ: {rel_path} ({proc_time:.2f}s, å¹³å‡: {avg_time:.2f}s)")
        else:
            print(f"âŒ å¤„ç†å¤±è´¥: {rel_path}")
    
    print("=" * 60)
    print(f"ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!")
    print(f"  - æˆåŠŸå¤„ç†: {success_count}/{len(image_files)} ä¸ªæ–‡ä»¶")
    print(f"  - æ€»ç”¨æ—¶: {total_time:.2f}s")
    if success_count > 0:
        print(f"  - å¹³å‡æ¯å¼ : {total_time/success_count:.2f}s")
    print(f"  - è¾“å‡ºç›®å½•: {args.output_dir}")

def main():
    parser = argparse.ArgumentParser(description="Face Anonymization Made Simple - ä¼˜åŒ–æ‰¹é‡å¤„ç†å·¥å…·")
    parser.add_argument("--input_dir", required=True, help="è¾“å…¥å›¾åƒç›®å½•")
    parser.add_argument("--output_dir", required=True, help="è¾“å‡ºå›¾åƒç›®å½•")
    parser.add_argument("--pattern", default="**/*.jpg", help="å›¾åƒæ–‡ä»¶åŒ¹é…æ¨¡å¼ (é»˜è®¤: '**/*.jpg')")
    parser.add_argument("--anonymization_degree", type=float, default=1.25, help="åŒ¿ååŒ–ç¨‹åº¦ (é»˜è®¤: 1.25)")
    parser.add_argument("--num_inference_steps", type=int, default=25, help="æ¨ç†æ­¥æ•° (é»˜è®¤: 25, åŸå§‹200)")
    parser.add_argument("--guidance_scale", type=float, default=4.0, help="å¼•å¯¼ç¼©æ”¾ (é»˜è®¤: 4.0)")
    parser.add_argument("--use_fp16", action="store_true", help="ä½¿ç”¨åŠç²¾åº¦æµ®ç‚¹æ•°ä»¥èŠ‚çœæ˜¾å­˜")
    parser.add_argument("--overwrite", action="store_true", help="è¦†ç›–å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶")
    
    args = parser.parse_args()
    
    print("ğŸš€ Face Anonymization Made Simple - ä¼˜åŒ–æ‰¹é‡å¤„ç†å·¥å…·")
    print("=" * 60)
    print(f"âš™ï¸ é…ç½®:")
    print(f"  - è¾“å…¥ç›®å½•: {args.input_dir}")
    print(f"  - è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"  - åŒ¹é…æ¨¡å¼: {args.pattern}")
    print(f"  - åŒ¿ååŒ–ç¨‹åº¦: {args.anonymization_degree}")
    print(f"  - æ¨ç†æ­¥æ•°: {args.num_inference_steps} (åŸå§‹: 200)")
    print(f"  - å¼•å¯¼ç¼©æ”¾: {args.guidance_scale}")
    print(f"  - ä½¿ç”¨åŠç²¾åº¦: {args.use_fp16}")
    print(f"  - è¦†ç›–æ–‡ä»¶: {args.overwrite}")
    print(f"  - CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"  - GPUè®¾å¤‡: {torch.cuda.get_device_name()}")
        print(f"  - GPUæ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    print("=" * 60)
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not os.path.exists(args.input_dir):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input_dir}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # å¼€å§‹å¤„ç†
    process_batch_optimized(args)

if __name__ == "__main__":
    main()
