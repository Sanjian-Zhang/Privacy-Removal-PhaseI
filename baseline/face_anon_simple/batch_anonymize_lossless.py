#!/usr/bin/env python3
"""
Face Anonymization Made Simple - Lossless batch anonymization processing script
Maintain original image dimensions and highest quality
"""

import os
import argparse
import glob
import torch
from tqdm import tqdm
from pathlib import Path
from PIL import Image
import time

try:
    from transformers import CLIPImageProcessor, CLIPVisionModel
    from diffusers import AutoencoderKL, DDPMScheduler
    from diffusers.utils import load_image
except ImportError as e:
    print(f"âŒ Import error: {e}")
    print("Please ensure required dependencies are installed: pip install transformers diffusers")
from src.diffusers.models.referencenet.referencenet_unet_2d_condition import (
    ReferenceNetModel,
)
from src.diffusers.models.referencenet.unet_2d_condition import UNet2DConditionModel
from src.diffusers.pipelines.referencenet.pipeline_referencenet import (
    StableDiffusionReferenceNetPipeline,
)
import time

def setup_pipeline(use_fp16=False, enable_attention_slicing=True):
    """Setup high quality processing pipeline - lossless mode uses fp32"""
    print("ğŸ”§ Setting up high quality processing pipeline (lossless mode)...")
    
    face_model_id = "hkung/face-anon-simple"
    clip_model_id = "openai/clip-vit-large-patch14"
    sd_model_id = "stabilityai/stable-diffusion-2-1"

    # Use fp32 to ensure highest quality
    torch_dtype = torch.float32
    
    print("  - Loading UNet model...")
    unet = UNet2DConditionModel.from_pretrained(
        face_model_id, subfolder="unet", use_safetensors=True, torch_dtype=torch_dtype
    )
    
    print("  - Loading ReferenceNet model...")
    referencenet = ReferenceNetModel.from_pretrained(
        face_model_id, subfolder="referencenet", use_safetensors=True, torch_dtype=torch_dtype
    )
    
    print("  - Loading Conditioning ReferenceNet model...")
    conditioning_referencenet = ReferenceNetModel.from_pretrained(
        face_model_id, subfolder="conditioning_referencenet", use_safetensors=True, torch_dtype=torch_dtype
    )
    
    print("  - Loading VAE...")
    vae = AutoencoderKL.from_pretrained(
        sd_model_id, subfolder="vae", use_safetensors=True, torch_dtype=torch_dtype
    )
    
    print("  - åŠ è½½Scheduler...")
    scheduler = DDPMScheduler.from_pretrained(
        sd_model_id, subfolder="scheduler", use_safetensors=True
    )
    
    print("  - åŠ è½½CLIPç‰¹å¾æå–å™¨...")
    feature_extractor = CLIPImageProcessor.from_pretrained(clip_model_id)
    
    print("  - åŠ è½½CLIPå›¾åƒç¼–ç å™¨...")
    image_encoder = CLIPVisionModel.from_pretrained(
        clip_model_id, torch_dtype=torch_dtype, use_safetensors=True
    )
    
    # åˆ›å»ºç®¡é“
    pipe = StableDiffusionReferenceNetPipeline(
        unet=unet,
        referencenet=referencenet,
        conditioning_referencenet=conditioning_referencenet,
        vae=vae,
        scheduler=scheduler,
        feature_extractor=feature_extractor,
        image_encoder=image_encoder,
    )
    
    # å¯ç”¨å†…å­˜ä¼˜åŒ–
    if enable_attention_slicing:
        pipe.enable_attention_slicing()
    
    # å¯ç”¨æ›´å¤šå†…å­˜ä¼˜åŒ–
    try:
        pipe.enable_model_cpu_offload()
        print("  âœ… å¯ç”¨æ¨¡å‹CPUå¸è½½")
    except:
        print("  âš ï¸ æ— æ³•å¯ç”¨æ¨¡å‹CPUå¸è½½")
    
    try:
        pipe.enable_sequential_cpu_offload()
        print("  âœ… å¯ç”¨é¡ºåºCPUå¸è½½")
    except:
        print("  âš ï¸ æ— æ³•å¯ç”¨é¡ºåºCPUå¸è½½")
    
    # ç§»è‡³GPU
    if torch.cuda.is_available():
        device = "cuda"
        print("âœ… ä½¿ç”¨GPUå¤„ç†")
        
        # æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()
        print("  âœ… æ¸…ç†GPUç¼“å­˜")
    else:
        device = "cpu"
        print("âš ï¸ æœªæ£€æµ‹åˆ°CUDAï¼Œä½¿ç”¨CPUå¤„ç†")
    
    return pipe

def anonymize_image_lossless(pipe, image_path, output_path, 
                        anonymization_degree=1.25, 
                        num_inference_steps=50,  # æé«˜æ¨ç†æ­¥æ•°ç¡®ä¿è´¨é‡
                        guidance_scale=4.0):
    """æ— æŸåŒ¿ååŒ–å•ä¸ªå›¾åƒ"""
    try:
        # åŠ è½½å›¾åƒ
        original_image = load_image(image_path)
        
        # ä¿æŒåŸå§‹å›¾ç‰‡å°ºå¯¸ï¼Œä½†å¯¹äºè¶…å¤§å›¾ç‰‡è¿›è¡Œæ™ºèƒ½ç¼©æ”¾ä»¥é¿å…æ˜¾å­˜ä¸è¶³
        w, h = original_image.size
        original_w, original_h = w, h
        
        # è®¡ç®—åƒç´ æ•°é‡ï¼Œå¦‚æœè¶…è¿‡é˜ˆå€¼åˆ™ç¼©æ”¾
        max_pixels = 1920 * 1080  # 2Måƒç´ é˜ˆå€¼ï¼Œé€‚åˆå¤§å¤šæ•°GPU
        current_pixels = w * h
        
        if current_pixels > max_pixels:
            # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
            scale_factor = (max_pixels / current_pixels) ** 0.5
            new_w = int(w * scale_factor)
            new_h = int(h * scale_factor)
            print(f"  å›¾ç‰‡è¿‡å¤§ï¼Œä¸´æ—¶ç¼©æ”¾: {w}x{h} â†’ {new_w}x{new_h} (ç¼©æ”¾æ¯”ä¾‹: {scale_factor:.3f})")
        else:
            new_w, new_h = w, h
        
        # ç¡®ä¿å°ºå¯¸æ˜¯8çš„å€æ•°ï¼ˆæ‰©æ•£æ¨¡å‹è¦æ±‚ï¼‰
        new_w = (new_w // 8) * 8
        new_h = (new_h // 8) * 8
        
        # å¦‚æœè°ƒæ•´åçš„å°ºå¯¸ä¸º0ï¼Œè®¾ç½®æœ€å°å€¼
        if new_w == 0:
            new_w = 8
        if new_h == 0:
            new_h = 8
        
        print(f"  å¤„ç†å°ºå¯¸: {new_w}x{new_h}")
        
        # ç”ŸæˆåŒ¿ååŒ–å›¾åƒ
        generator = torch.manual_seed(42)
        
        start_time = time.time()
        anon_image = pipe(
            source_image=original_image,
            conditioning_image=original_image,
            num_inference_steps=num_inference_steps,  # ä½¿ç”¨æ›´å¤šæ¨ç†æ­¥æ•°
            guidance_scale=guidance_scale,
            generator=generator,
            anonymization_degree=anonymization_degree,
            width=new_w,
            height=new_h,
        ).images[0]
        
        processing_time = time.time() - start_time
        
        # è°ƒæ•´å›åŸå§‹å°ºå¯¸ï¼ˆä½¿ç”¨é«˜è´¨é‡é‡é‡‡æ ·ï¼‰
        if (new_w, new_h) != (original_w, original_h):
            print(f"  è°ƒæ•´å›åŸå§‹å°ºå¯¸: {original_w}x{original_h}")
            anon_image = anon_image.resize((original_w, original_h), Image.Resampling.LANCZOS)
        
        # ä¿å­˜ç»“æœ - æ— æŸä¿å­˜
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # æ ¹æ®åŸå§‹æ ¼å¼é€‰æ‹©ä¿å­˜æ–¹å¼
        original_ext = os.path.splitext(image_path)[1].lower()
        if original_ext in ['.png', '.tiff', '.tif']:
            # æ— æŸæ ¼å¼
            output_path_lossless = os.path.splitext(output_path)[0] + original_ext
            anon_image.save(output_path_lossless, format='PNG' if original_ext == '.png' else 'TIFF')
            print(f"  ä¿å­˜ä¸ºæ— æŸæ ¼å¼: {output_path_lossless}")
        else:
            # JPEGæ ¼å¼ä½¿ç”¨æœ€é«˜è´¨é‡
            anon_image.save(output_path, format='JPEG', quality=100, optimize=False, subsampling=0)
            print(f"  ä¿å­˜ä¸ºé«˜è´¨é‡JPEG: {output_path}")
        
        return True, processing_time
    except Exception as e:
        print(f"âŒ å¤„ç† {image_path} å¤±è´¥: {str(e)}")
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        return False, 0

def process_batch_lossless(args):
    """æ— æŸæ‰¹é‡å¤„ç†å›¾åƒ"""
    # è·å–æ‰€æœ‰è¾“å…¥æ–‡ä»¶
    input_pattern = os.path.join(args.input_dir, args.pattern)
    image_files = glob.glob(input_pattern, recursive=True)
    
    if not image_files:
        print(f"âŒ åœ¨ {input_pattern} ä¸­æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(image_files)} ä¸ªå›¾åƒæ–‡ä»¶")
    
    # è®¾ç½®ç®¡é“
    pipe = setup_pipeline(use_fp16=False)  # æ— æŸæ¨¡å¼ä¸ä½¿ç”¨fp16
    
    # ç»Ÿè®¡ä¿¡æ¯
    success_count = 0
    total_time = 0
    
    # å¤„ç†æ¯ä¸ªå›¾åƒ
    for image_file in tqdm(image_files, desc="å¤„ç†å›¾åƒ"):
        # ç”Ÿæˆè¾“å‡ºè·¯å¾„
        rel_path = os.path.relpath(image_file, args.input_dir)
        output_path = os.path.join(args.output_dir, rel_path)
        
        # æ£€æŸ¥æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶
        if os.path.exists(output_path) and not args.overwrite:
            print(f"â­ï¸ è·³è¿‡å·²å­˜åœ¨æ–‡ä»¶: {output_path}")
            continue
        
        # å¤„ç†å›¾åƒ
        success, proc_time = anonymize_image_lossless(
            pipe, image_file, output_path,
            anonymization_degree=args.anonymization_degree,
            num_inference_steps=args.num_inference_steps,
            guidance_scale=args.guidance_scale
        )
        
        if success:
            success_count += 1
            total_time += proc_time
            avg_time = total_time / success_count
            print(f"âœ… å¤„ç†å®Œæˆ: {rel_path} ({proc_time:.2f}s, å¹³å‡: {avg_time:.2f}s)")
        else:
            print(f"âŒ å¤„ç†å¤±è´¥: {rel_path}")
    
    print("=" * 60)
    print(f"ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!")
    print(f"  - æˆåŠŸå¤„ç†: {success_count}/{len(image_files)} ä¸ªæ–‡ä»¶")
    print(f"  - æ€»ç”¨æ—¶: {total_time:.2f}s")
    if success_count > 0:
        print(f"  - å¹³å‡æ¯å¼ : {total_time/success_count:.2f}s")
    print(f"  - è¾“å‡ºç›®å½•: {args.output_dir}")

def main():
    parser = argparse.ArgumentParser(description="Face Anonymization Made Simple - æ— æŸæ‰¹é‡å¤„ç†å·¥å…·")
    parser.add_argument("--input_dir", required=True, help="è¾“å…¥å›¾åƒç›®å½•")
    parser.add_argument("--output_dir", required=True, help="è¾“å‡ºå›¾åƒç›®å½•")
    parser.add_argument("--pattern", default="**/*.jpg", help="å›¾åƒæ–‡ä»¶åŒ¹é…æ¨¡å¼ (é»˜è®¤: '**/*.jpg')")
    parser.add_argument("--anonymization_degree", type=float, default=1.25, help="åŒ¿ååŒ–ç¨‹åº¦ (é»˜è®¤: 1.25)")
    parser.add_argument("--num_inference_steps", type=int, default=50, help="æ¨ç†æ­¥æ•° (é»˜è®¤: 50, é«˜è´¨é‡æ¨¡å¼)")
    parser.add_argument("--guidance_scale", type=float, default=4.0, help="å¼•å¯¼ç¼©æ”¾ (é»˜è®¤: 4.0)")
    parser.add_argument("--overwrite", action="store_true", help="è¦†ç›–å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶")
    
    args = parser.parse_args()
    
    print("ğŸš€ Face Anonymization Made Simple - æ— æŸæ‰¹é‡å¤„ç†å·¥å…·")
    print("=" * 60)
    print(f"âš™ï¸ é…ç½®:")
    print(f"  - è¾“å…¥ç›®å½•: {args.input_dir}")
    print(f"  - è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"  - åŒ¹é…æ¨¡å¼: {args.pattern}")
    print(f"  - åŒ¿ååŒ–ç¨‹åº¦: {args.anonymization_degree}")
    print(f"  - æ¨ç†æ­¥æ•°: {args.num_inference_steps} (é«˜è´¨é‡æ¨¡å¼)")
    print(f"  - å¼•å¯¼ç¼©æ”¾: {args.guidance_scale}")
    print(f"  - è´¨é‡æ¨¡å¼: æ— æŸæ¨¡å¼ (FP32)")
    print(f"  - è¦†ç›–æ–‡ä»¶: {args.overwrite}")
    print(f"  - CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"  - GPUè®¾å¤‡: {torch.cuda.get_device_name()}")
        print(f"  - GPUæ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    print("=" * 60)
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not os.path.exists(args.input_dir):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input_dir}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # å¼€å§‹å¤„ç†
    process_batch_lossless(args)

if __name__ == "__main__":
    main()
