#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
cuDNNå…¼å®¹æ€§ä¿®å¤å·¥å…·
è§£å†³ CUDNN_STATUS_SUBLIBRARY_VERSION_MISMATCH é”™è¯¯
"""

import os
import sys
import logging
from pathlib import Path

def fix_cudnn_compatibility():
    """ä¿®å¤cuDNNå…¼å®¹æ€§é—®é¢˜"""
    print("ğŸ”§ æ­£åœ¨ä¿®å¤cuDNNå…¼å®¹æ€§...")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    env_fixes = {
        'TF_ENABLE_ONEDNN_OPTS': '0',
        'CUDA_VISIBLE_DEVICES': '0',
        'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:128',
        'TF_CPP_MIN_LOG_LEVEL': '2',  # å‡å°‘TensorFlowæ—¥å¿—
        'CUDA_LAUNCH_BLOCKING': '1',  # åŒæ­¥CUDAæ“ä½œä¾¿äºè°ƒè¯•
    }
    
    for key, value in env_fixes.items():
        os.environ[key] = value
        print(f"  âœ… è®¾ç½® {key}={value}")
    
    try:
        import torch
        if torch.cuda.is_available():
            # æ¸…ç†GPUç¼“å­˜
            torch.cuda.empty_cache()
            device_count = torch.cuda.device_count()
            print(f"  ğŸš€ æ£€æµ‹åˆ° {device_count} ä¸ªGPUè®¾å¤‡")
            
            for i in range(device_count):
                name = torch.cuda.get_device_name(i)
                memory = torch.cuda.get_device_properties(i).total_memory / 1e9
                print(f"    GPU {i}: {name} ({memory:.1f}GB)")
            
            # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥
            if hasattr(torch.cuda, 'set_per_process_memory_fraction'):
                torch.cuda.set_per_process_memory_fraction(0.8)  # é™åˆ¶ä½¿ç”¨80%æ˜¾å­˜
                print("  âš™ï¸ è®¾ç½®GPUå†…å­˜ä½¿ç”¨é™åˆ¶ä¸º80%")
        else:
            print("  âš ï¸ CUDAä¸å¯ç”¨")
            
    except ImportError:
        print("  âš ï¸ PyTorchæœªå®‰è£…")
    
    # å°è¯•ä¿®å¤å¸¸è§çš„cuDNNé—®é¢˜
    try:
        import torch.backends.cudnn as cudnn
        if torch.cuda.is_available():
            # ç¦ç”¨benchmarkæ¨¡å¼å¯èƒ½è§£å†³ç‰ˆæœ¬ä¸åŒ¹é…é—®é¢˜
            cudnn.benchmark = False
            cudnn.deterministic = True
            print("  ğŸ”’ è®¾ç½®cuDNNä¸ºç¡®å®šæ€§æ¨¡å¼")
    except Exception as e:
        print(f"  âš ï¸ cuDNNè®¾ç½®è­¦å‘Š: {e}")

def create_conda_environment_script():
    """åˆ›å»ºcondaç¯å¢ƒè®¾ç½®è„šæœ¬"""
    script_content = """#!/bin/bash
# cuDNNå…¼å®¹æ€§ä¿®å¤è„šæœ¬

echo "ğŸ”§ ä¿®å¤cuDNNå…¼å®¹æ€§é—®é¢˜..."

# è®¾ç½®ç¯å¢ƒå˜é‡
export TF_ENABLE_ONEDNN_OPTS=0
export CUDA_VISIBLE_DEVICES=0
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128
export TF_CPP_MIN_LOG_LEVEL=2

echo "âœ… ç¯å¢ƒå˜é‡å·²è®¾ç½®"

# æ£€æŸ¥CUDAçŠ¶æ€
if command -v nvidia-smi &> /dev/null; then
    echo "ğŸš€ GPUä¿¡æ¯:"
    nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
else
    echo "âš ï¸ nvidia-smi æœªæ‰¾åˆ°"
fi

echo "ğŸ¯ ç°åœ¨å¯ä»¥è¿è¡ŒPythonè„šæœ¬äº†"
"""
    
    script_path = Path("/home/zhiqics/sanjian/predata/fix_cudnn.sh")
    with open(script_path, "w") as f:
        f.write(script_content)
    
    # æ·»åŠ æ‰§è¡Œæƒé™
    import stat
    script_path.chmod(script_path.stat().st_mode | stat.S_IEXEC)
    
    print(f"âœ… åˆ›å»ºcuDNNä¿®å¤è„šæœ¬: {script_path}")
    print("ä½¿ç”¨æ–¹æ³•: source fix_cudnn.sh && python your_script.py")

def check_video_file(video_path: str):
    """æ£€æŸ¥è§†é¢‘æ–‡ä»¶çŠ¶æ€"""
    path = Path(video_path)
    
    print(f"\nğŸ” æ£€æŸ¥è§†é¢‘æ–‡ä»¶: {path}")
    
    if not path.exists():
        print("âŒ æ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    # æ–‡ä»¶å¤§å°
    size_mb = path.stat().st_size / (1024 * 1024)
    print(f"ğŸ“ æ–‡ä»¶å¤§å°: {size_mb:.1f} MB")
    
    # æ–‡ä»¶å¤´æ£€æŸ¥
    try:
        with open(path, 'rb') as f:
            header = f.read(100)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„MP4æ–‡ä»¶
        if b'ftyp' in header[:20]:
            print("âœ… æ–‡ä»¶å¤´æ­£å¸¸ (MP4æ ¼å¼)")
        else:
            print("âš ï¸ æ–‡ä»¶å¤´å¼‚å¸¸")
            print(f"å‰20å­—èŠ‚: {header[:20].hex()}")
    
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return False
    
    # ä½¿ç”¨ffprobeæ£€æŸ¥
    try:
        import subprocess
        cmd = ["ffprobe", "-v", "quiet", "-print_format", "json", "-show_format", str(path)]
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        
        if result.returncode == 0:
            import json
            info = json.loads(result.stdout)
            if 'format' in info:
                duration = info['format'].get('duration', 'unknown')
                format_name = info['format'].get('format_name', 'unknown')
                print(f"âœ… ffprobeæ£€æŸ¥é€šè¿‡: æ ¼å¼={format_name}, æ—¶é•¿={duration}s")
                return True
        
        print(f"âŒ ffprobeå¤±è´¥: {result.stderr}")
        return False
        
    except Exception as e:
        print(f"âŒ ffprobeæ£€æŸ¥å¼‚å¸¸: {e}")
        return False

def main():
    print("ğŸš€ cuDNNå…¼å®¹æ€§ä¿®å¤å·¥å…·")
    print("=" * 50)
    
    # ä¿®å¤cuDNN
    fix_cudnn_compatibility()
    
    # åˆ›å»ºä¿®å¤è„šæœ¬
    create_conda_environment_script()
    
    # æ£€æŸ¥é—®é¢˜è§†é¢‘
    problem_videos = [
        "/home/zhiqics/sanjian/predata/downloaded_video36.mp4",
        "/home/zhiqics/sanjian/predata/downloaded_video38.mp4"
    ]
    
    for video in problem_videos:
        check_video_file(video)
    
    print("\n" + "=" * 50)
    print("ğŸ¯ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
    print("1. video36 å’Œ video38 å¯èƒ½ä¸‹è½½æŸåï¼Œå»ºè®®é‡æ–°ä¸‹è½½")
    print("2. ä½¿ç”¨ video40 æµ‹è¯•æŠ½å¸§åŠŸèƒ½")
    print("3. è¿è¡Œå‰å…ˆæ‰§è¡Œ: source fix_cudnn.sh")
    print("4. å¦‚æœGPUé—®é¢˜æŒç»­ï¼Œå¯ä»¥æ·»åŠ  --no-gpu å‚æ•°ä½¿ç”¨CPU")

if __name__ == "__main__":
    main()
