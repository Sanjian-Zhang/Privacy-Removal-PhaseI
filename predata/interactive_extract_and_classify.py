#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import json
import gc
import re
import time
import shutil
import logging
import subprocess
import tempfile
from pathlib import Path
from typing import Optional, Dict, List, Tuple


import importlib.util
import numpy as np
import cv2
# ========= ÂàÜÁ±ªÂô®Âä®ÊÄÅÂä†ËΩΩ‰∏éË∞ÉÁî® =========
def load_classifier_module(module_path: Path):
    """Êää face_plate_classifier.py ÂΩìÊ®°ÂùóÂä†ËΩΩÔºà‰∏çÈúÄË¶ÅÊîπ‰Ω†ÂéüÊñá‰ª∂Ôºâ"""
    if not module_path.exists():
        print(f"‚ùå ÂàÜÁ±ªÂô®Ê®°Âùó‰∏çÂ≠òÂú®: {module_path}"); sys.exit(1)
    spec = importlib.util.spec_from_file_location("face_plate_classifier", str(module_path))
    mod = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(mod)
    return mod

def run_classifier(mod, frames_dir: Path, plate_model_path: Path):
    """
    ÂÖ≥ÈîÆÁÇπÔºöÂú®ÂÆû‰æãÂåñ FacePlateClassifier ‰πãÂâçÔºåÁõ¥Êé•ÈáçÂÜôÂÆÉÊ®°ÂùóÁ∫ßÂÖ®Â±ÄÂ∏∏ÈáèÔºå
    Ëß£ÂÜ≥‰Ω†ÂéüËÑöÊú¨Èáå INPUT_DIR/OUTPUT_BASE_DIR/Ê®°ÂûãË∑ØÂæÑÂÜôÊ≠ªÁöÑÈóÆÈ¢ò„ÄÇ
    """
    # 1) ÊîπÂÜôËæìÂÖ•/ËæìÂá∫‰∏éÊ®°ÂûãË∑ØÂæÑ
    mod.INPUT_DIR = str(frames_dir)
    mod.OUTPUT_BASE_DIR = str(frames_dir)
    mod.PLATE_MODEL_PATH = str(plate_model_path)

    # 2) ÂêåÊ≠•ÁõÆÂΩïÂ∏∏ÈáèÔºàÂÆÉ‰ª¨Âú®ÂéüÊ®°ÂùóÈáåÊòØÂü∫‰∫é OUTPUT_BASE_DIR ÂÆö‰πâÁöÑÔºâ
    mod.QUALIFIED_DIR = os.path.join(mod.OUTPUT_BASE_DIR, "qualified")
    mod.INSUFFICIENT_SCORE_DIR = os.path.join(mod.OUTPUT_BASE_DIR, "insufficient_score")
    mod.NO_CONTENT_DIR = os.path.join(mod.OUTPUT_BASE_DIR, "no_content")
    mod.ANALYSIS_DIR = os.path.join(mod.OUTPUT_BASE_DIR, "analysis")
    for d in [mod.QUALIFIED_DIR, mod.INSUFFICIENT_SCORE_DIR, mod.NO_CONTENT_DIR, mod.ANALYSIS_DIR]:
        os.makedirs(d, exist_ok=True)

    # 3) ËøêË°å
    clf = mod.FacePlateClassifier()
    clf.run()

# ========= ÈÄöÁî®Â∑•ÂÖ∑ =========
def which_or_die(bin_name: str):
    path = shutil.which(bin_name)
    if not path:
        print(f"‚ùå Êú™ÊâæÂà∞‰æùËµñÔºö{bin_name}ÔºàËØ∑ÂÖàÂÆâË£ÖÔºâ")
        sys.exit(1)
    return path

def run(cmd, timeout=None, check=False, capture=False):
    """Áªü‰∏ÄÊâßË°åÂëΩ‰ª§ÔºõÂØπ ffmpeg/ffprobe ÈôçÂô™‰ΩÜ‰øùÁïô stats„ÄÇ"""
    if isinstance(cmd, (list, tuple)) and cmd:
        head = str(cmd[0])
        if head.endswith("ffmpeg") or head.endswith("ffprobe") or head in ("ffmpeg", "ffprobe"):
            if "-hide_banner" not in cmd:
                cmd.insert(1, "-hide_banner")
            if "-loglevel" not in cmd:
                cmd.insert(1, "-loglevel"); cmd.insert(2, "error")
            if (head.endswith("ffmpeg") or head == "ffmpeg") and "-stats" not in cmd:
                cmd.insert(3, "-stats")
    print("üöÄ", " ".join(str(x) for x in cmd))
    return subprocess.run(cmd, timeout=timeout, check=check, text=True, capture_output=capture)

def atomic_move(src: Path, dst: Path):
    dst.parent.mkdir(parents=True, exist_ok=True)
    try:
        os.replace(src, dst)
    except OSError as e:
        if e.errno == 18:  # cross-device
            shutil.copy2(src, dst); src.unlink()
        else:
            raise

# ========= ffmpeg ÂÖºÂÆπÂÖúÂ∫ïÔºàH.264/MP4Ôºâ =========
def ffprobe_json(path: Path):
    which_or_die("ffprobe")
    res = run([
        "ffprobe", "-v", "error",
        "-select_streams", "v:0",
        "-show_entries", "format=format_name:stream=codec_name,avg_frame_rate,nb_frames",
        "-of", "json", str(path)
    ], capture=True)
    if res.returncode != 0:
        return None
    try:
        return json.loads(res.stdout or "{}")
    except Exception:
        return None

def _transcode_to_h264(path: Path):
    which_or_die("ffmpeg")
    out = path.with_suffix(".h264.mp4")
    res = run([
        "ffmpeg", "-i", str(path),
        "-c:v", "libx264", "-pix_fmt", "yuv420p", "-preset", "veryfast",
        "-c:a", "aac", "-b:a", "192k",
        "-movflags", "+faststart", "-y", str(out)
    ])
    if res.returncode != 0 or (not out.exists()) or out.stat().st_size == 0:
        print("‚ùå ËΩ¨Á†ÅÂ§±Ë¥•"); sys.exit(1)
    atomic_move(out, path)
    print("üß© Â∑≤ËΩ¨Á†Å‰∏∫ H.264 MP4")

def ensure_ffmpeg_compatible(path: Path):
    info = ffprobe_json(path)
    if not info or "streams" not in info or not info["streams"]:
        print("‚ö†Ô∏è ffprobe Â§±Ë¥•ÔºåÁõ¥Êé•ËΩ¨Á†ÅÂà∞ H.264‚Ä¶")
        _transcode_to_h264(path); return
    fmt = (info.get("format", {}) or {}).get("format_name", "") or ""
    vcodec = (info["streams"][0] or {}).get("codec_name", "")
    is_mp4 = "mp4" in fmt
    is_h264 = (vcodec == "h264")
    if is_mp4 and is_h264:
        print("‚úÖ Â∑≤ÂÖºÂÆπÔºöMP4 + H.264"); return
    if (not is_mp4) and is_h264:
        fixed = path.with_suffix(".repack.mp4")
        res = run(["ffmpeg", "-i", str(path), "-c", "copy", "-movflags", "+faststart", "-y", str(fixed)])
        if res.returncode == 0 and fixed.exists() and fixed.stat().st_size > 0:
            atomic_move(fixed, path)
            print("üîÅ Â∑≤ÈáçÂ∞ÅË£Ö‰∏∫ MP4ÔºàÊó†ÈáçÂéãÁº©Ôºâ")
            return ensure_ffmpeg_compatible(path)
    _transcode_to_h264(path)

# ========= ‰∏ãËΩΩÔºà‰∏§Èò∂ÊÆµÔºö‰ºòÂÖà MP4+M4AÔºåÂÖ∂Ê¨°‰ªªÊÑèÈü≥È¢ë‚ÜíAACÔºâ =========
def download_video(url: str, out_dir: Path, video_number: str) -> Path:
    which_or_die("yt-dlp"); which_or_die("ffmpeg")
    out_dir.mkdir(parents=True, exist_ok=True)

    tmp_dir = Path(tempfile.mkdtemp(prefix="yt_"))
    stem = f"downloaded_video{video_number}"
    out_tmpl = str(tmp_dir / (stem + ".%(ext)s"))
    out_final = out_dir / f"{stem}.mp4"

    attempts = [
        ("bv*[ext=mp4]+ba[ext=m4a]/b[ext=mp4]", None, "‰ºòÂÖà MP4+M4A"),
        ("bv*+ba/b", "ffmpeg:-c:v copy -c:a aac -b:a 192k -movflags +faststart", "ÂÖúÂ∫ïÔºö‰ªªÊÑèÈü≥È¢ë‚ÜíAAC")
    ]
    for i, (fmt, pp, desc) in enumerate(attempts, 1):
        print(f"üéØ Â∞ùËØï {i}/{len(attempts)}Ôºö{desc}")
        cmd = [
            "yt-dlp", "-N", "8",
            "--no-part", "--continue", "--restrict-filenames",
            "--merge-output-format", "mp4",
            "-o", out_tmpl, "-f", fmt, url
        ]
        if pp:
            cmd += ["--postprocessor-args", pp]
        res = run(cmd)
        if res.returncode == 0:
            cand = None
            for f in tmp_dir.iterdir():
                if f.is_file() and f.suffix.lower() in (".mp4", ".mkv", ".mov"):
                    if ".f" in f.name or f.name.endswith(".ytdl"):  # ËøáÊª§ÂàÜËΩ®‰∏¥Êó∂Êñá‰ª∂
                        continue
                    cand = f
            if cand:
                atomic_move(cand, out_final)
                print(f"‚úÖ ‰∏ãËΩΩÂÆåÊàêÔºö{out_final}")
                shutil.rmtree(tmp_dir, ignore_errors=True)
                ensure_ffmpeg_compatible(out_final)
                return out_final
        print("‚ö†Ô∏è Êú¨Ê¨°ÊñπÊ°àÂ§±Ë¥•ÔºåÂàáÊç¢‰∏ã‰∏ÄÁßç‚Ä¶")

    shutil.rmtree(tmp_dir, ignore_errors=True)
    print("‚ùå ËßÜÈ¢ë‰∏ãËΩΩÂ§±Ë¥•"); sys.exit(1)

# ========= ÊäΩÂ∏ß =========
def extract_frames_ffmpeg(video_path: Path, output_dir: Path, fps: float, jpg_q: int, video_number: str) -> bool:
    which_or_die("ffmpeg")
    output_dir.mkdir(parents=True, exist_ok=True)
    pattern = output_dir / f"video{video_number}_frame_%06d.jpg"
    cmd = [
        "ffmpeg",
        "-i", str(video_path),
        "-vsync", "vfr",
        "-vf", f"fps={fps}",
        "-q:v", str(jpg_q),
        "-vcodec", "mjpeg",
        "-y", str(pattern)
    ]
    res = run(cmd)
    if res.returncode == 0:
        print(f"‚úÖ ÊäΩÂ∏ßÂÆåÊàêÔºö{output_dir}")
        return True
    else:
        print("‚ùå ÊäΩÂ∏ßÂ§±Ë¥•")
        return False

# ========= ÂàÜÁ±ª‰æùËµñÊ£ÄÊü• =========
def check_py_deps() -> bool:
    missing = []
    try:
        import torch  # noqa
    except Exception:
        missing.append("torch")
    try:
        from retinaface import RetinaFace  # noqa
    except Exception:
        missing.append("retina-face")
    try:
        from ultralytics import YOLO  # noqa
    except Exception:
        missing.append("ultralytics")
    try:
        import easyocr  # noqa
    except Exception:
        missing.append("easyocr")

    if missing:
        print("‚ùå Áº∫Â∞ë‰æùËµñÂ∫ìÔºö", ", ".join(missing))
        print("ËØ∑ÂÆâË£ÖÔºö")
        for dep in missing:
            print(f"  pip install {dep}")
        return False
    return True

# ========= Ê≠£ËÑ∏/ËΩ¶Áâå/OCR ËØÑÂàÜÂàÜÁ±ªÂô® =========
class FacePlateClassifier:
    SUPPORTED_FORMATS = ('.jpg', '.jpeg', '.png', '.bmp', '.webp')

    def __init__(self,
                 input_dir: str,
                 output_base: str,
                 plate_model_path: str,
                 score_threshold: int = 5,
                 clear_face_score: int = 2,
                 clear_plate_score: int = 2,
                 text_score: int = 2,
                 text_fields_threshold: int = 10,
                 yaw_thresh: float = 35.0,
                 min_face_conf: float = 0.8,
                 min_plate_conf: float = 0.5,
                 min_face_size: int = 60,
                 min_plate_size: int = 50,
                 min_face_clarity: float = 30.0,
                 face_area_px2: int = 3600,
                 face_dist_ratio: float = 0.3,
                 min_text_conf: float = 0.5,
                 min_text_len: int = 3,
                 use_gpu: bool = True,
                 gpu_id: int = 0,
                 enable_torch_opt: bool = True,
                 gc_frequency: int = 100,
                 progress_update_frequency: int = 50):
        self.cfg = locals().copy()  # ‰øùÂ≠òÈÖçÁΩÆ
        self.cfg.pop('self')

        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        self.logger = logging.getLogger("Classifier")
        self.start_time = time.time()
        self.stats = {'qualified': 0, 'insufficient_score': 0, 'no_content': 0, 'failed': 0}
        self.analysis_results: List[Dict] = []

        self.device = self._setup_gpu()
        self._create_output_dirs()
        self._initialize_models()
        self._initialize_ocr()

    # ----- init helpers -----
    def _setup_gpu(self) -> str:
        import torch
        try:
            if self.cfg["use_gpu"] and torch.cuda.is_available():
                cnt = torch.cuda.device_count()
                if self.cfg["gpu_id"] < cnt:
                    dev = f'cuda:{self.cfg["gpu_id"]}'
                    name = torch.cuda.get_device_name(self.cfg["gpu_id"])
                    mem = torch.cuda.get_device_properties(self.cfg["gpu_id"]).total_memory / 1024**3
                    self.logger.info(f"üöÄ GPU: {name} (id {self.cfg['gpu_id']})ÔºåÊòæÂ≠ò {mem:.1f} GB")
                    torch.cuda.empty_cache()
                    if self.cfg["enable_torch_opt"]:
                        torch.backends.cudnn.benchmark = True
                        torch.backends.cudnn.deterministic = False
                        self.logger.info("‚ö° Â∑≤ÂêØÁî® cuDNN benchmark")
                    return dev
                else:
                    self.logger.warning(f"‚ö†Ô∏è ÊåáÂÆö GPU_ID {self.cfg['gpu_id']} Ë∂ÖËåÉÂõ¥ÔºàÂÖ± {cnt} ÂùóÔºâ")
                    return 'cpu'
            else:
                if not torch.cuda.is_available():
                    self.logger.info("üíª Êú™Ê£ÄÊµãÂà∞ CUDAÔºå‰ΩøÁî® CPU")
                else:
                    self.logger.info("üíª Â∑≤Á¶ÅÁî® GPUÔºå‰ΩøÁî® CPU")
                return 'cpu'
        except Exception as e:
            self.logger.error(f"‚ùå GPU ËÆæÁΩÆÂ§±Ë¥•: {e}")
            return 'cpu'

    def _create_output_dirs(self):
        base = Path(self.cfg["output_base"])
        self.qual_dir = base / "qualified"
        self.insuf_dir = base / "insufficient_score"
        self.empty_dir = base / "no_content"
        self.analysis_dir = base / "analysis"
        for d in [self.qual_dir, self.insuf_dir, self.empty_dir, self.analysis_dir]:
            d.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"üìÅ ÂàõÂª∫ÁõÆÂΩï: {d}")

    def _initialize_models(self):
        from retinaface import RetinaFace
        from ultralytics import YOLO

        # RetinaFace ÁÆÄÊµã
        self.logger.info("üîç ÂàùÂßãÂåñ RetinaFace‚Ä¶")
        _ = RetinaFace.detect_faces(np.ones((100, 100, 3), dtype=np.uint8) * 128)
        self.retina = RetinaFace
        self.logger.info("‚úÖ RetinaFace Â∞±Áª™")

        # YOLO ËΩ¶Áâå
        plate_path = self.cfg["plate_model_path"]
        if not Path(plate_path).exists():
            raise FileNotFoundError(f"ËΩ¶ÁâåÊ®°Âûã‰∏çÂ≠òÂú®: {plate_path}")
        self.logger.info("üöó ÂàùÂßãÂåñ YOLO ËΩ¶ÁâåÊ®°Âûã‚Ä¶")
        self.plate_model = YOLO(plate_path)
        self.logger.info("‚úÖ YOLO ËΩ¶ÁâåÊ®°ÂûãÂ∞±Áª™")

    def _initialize_ocr(self):
        import easyocr
        gpu_flag = 'cuda' in self.device
        self.logger.info("üìù ÂàùÂßãÂåñ EasyOCR‚Ä¶")
        self.ocr_reader = easyocr.Reader(['ch_sim', 'en'], gpu=gpu_flag)
        self.logger.info(f"‚úÖ EasyOCR Â∞±Áª™Ôºà{'GPU' if gpu_flag else 'CPU'}Ôºâ")

    # ----- metrics helpers -----
    def _calc_face_clarity(self, image: np.ndarray, bbox: Tuple[int, int, int, int]) -> float:
        try:
            x1, y1, x2, y2 = bbox
            h, w = image.shape[:2]
            x1, y1 = max(0, int(x1)), max(0, int(y1))
            x2, y2 = min(w, int(x2)), min(h, int(y2))
            if x2 <= x1 or y2 <= y1:
                return 0.0
            roi = image[y1:y2, x1:x2]
            if roi.size == 0:
                return 0.0
            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY) if len(roi.shape) == 3 else roi
            lap_var = cv2.Laplacian(gray, cv2.CV_64F).var()
            gx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
            gy = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
            grad_mean = np.mean(np.sqrt(gx**2 + gy**2))
            return float(lap_var + 0.1 * grad_mean)
        except Exception:
            return 0.0

    def _calc_yaw(self, landmarks: Dict) -> float:
        try:
            left_eye = np.array(landmarks['left_eye'])
            right_eye = np.array(landmarks['right_eye'])
            nose = np.array(landmarks['nose'])
            eye_center = (left_eye + right_eye) / 2
            eye_w = np.linalg.norm(right_eye - left_eye)
            if eye_w < 10:
                return 90.0
            yaw = abs((nose[0] - eye_center[0]) / eye_w) * 60.0
            return float(yaw)
        except Exception:
            return 90.0

    # ----- detectors -----
    def _faces(self, image_path: str) -> Tuple[int, List[Dict]]:
        dets = self.retina.detect_faces(image_path)
        if not isinstance(dets, dict) or len(dets) == 0:
            return 0, []
        img = cv2.imread(image_path)
        if img is None:
            return 0, []
        H, W = img.shape[:2]
        img_area = W * H

        out = []
        for _, fd in dets.items():
            conf = float(fd.get('score', 0.0))
            if conf < self.cfg["min_face_conf"]:
                continue
            if 'facial_area' not in fd or 'landmarks' not in fd:
                continue
            x1, y1, x2, y2 = fd['facial_area']
            fw, fh = x2 - x1, y2 - y1
            if min(fw, fh) < self.cfg["min_face_size"]:
                continue

            clarity = self._calc_face_clarity(img, (x1, y1, x2, y2))
            area = fw * fh
            dist_ratio = area / img_area
            is_clear = clarity >= self.cfg["min_face_clarity"]
            is_close = dist_ratio >= self.cfg["face_dist_ratio"]
            is_big = area >= self.cfg["face_area_px2"]
            if not (is_clear and (is_close or is_big)):
                continue

            yaw = self._calc_yaw(fd['landmarks'])
            is_frontal = yaw <= self.cfg["yaw_thresh"]
            if is_frontal:
                out.append({
                    "confidence": conf,
                    "yaw_angle": yaw,
                    "is_frontal": True,
                    "facial_area": [int(x1), int(y1), int(x2), int(y2)],
                    "face_size": [int(fw), int(fh)],
                    "quality_info": {
                        "clarity_score": clarity,
                        "distance_score": dist_ratio,
                        "face_area": int(area),
                        "is_clear": is_clear, "is_close": is_close, "is_large_enough": is_big
                    }
                })
        return len(out), out

    def _plates(self, image_path: str) -> Tuple[int, List[Dict]]:
        rs = self.plate_model(image_path, verbose=False)
        if not rs or len(rs) == 0:
            return 0, []
        r0 = rs[0]
        if r0.boxes is None or len(r0.boxes) == 0:
            return 0, []
        out = []
        for box in r0.boxes:
            conf = float(box.conf[0])
            if conf < self.cfg["min_plate_conf"]:
                continue
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            w, h = x2 - x1, y2 - y1
            if min(w, h) < self.cfg["min_plate_size"]:
                continue
            out.append({"confidence": conf,
                        "bbox": [float(x1), float(y1), float(x2), float(y2)],
                        "plate_size": [float(w), float(h)]})
        return len(out), out

    def _texts(self, image_path: str) -> Tuple[int, List[Dict]]:
        img = cv2.imread(image_path)
        if img is None:
            return 0, []
        rs = self.ocr_reader.readtext(img)
        if not rs:
            return 0, []
        out = []
        for bbox, text, conf in rs:
            conf = float(conf) if conf is not None else 0.0
            if conf < self.cfg["min_text_conf"]:
                continue
            t = (text or "").strip()
            if len(t) < self.cfg["min_text_len"]:
                continue
            if t.replace(' ', '').replace('.', '').replace('-', '').replace('_', ''):
                out.append({"text": t, "confidence": conf, "bbox": bbox})
        return len(out), out

    def classify_image(self, image_path: str) -> Tuple[str, Dict]:
        try:
            fname = os.path.basename(image_path)
            n_face, face_det = self._faces(image_path)
            n_plate, plate_det = self._plates(image_path)
            n_text, text_det = self._texts(image_path)

            score = 0
            details = []
            if n_face > 0:
                s = n_face * self.cfg["clear_face_score"]; score += s
                details.append(f"Ê∏ÖÊô∞Ê≠£ËÑ∏ {n_face} Âº† √ó {self.cfg['clear_face_score']} = {s} ÂàÜ")
            if n_plate > 0:
                s = n_plate * self.cfg["clear_plate_score"]; score += s
                details.append(f"Ê∏ÖÊô∞ËΩ¶Áâå {n_plate} Âº† √ó {self.cfg['clear_plate_score']} = {s} ÂàÜ")
            if n_text >= self.cfg["text_fields_threshold"]:
                s = self.cfg["text_score"]; score += s
                details.append(f"ÂèØËØÜÂà´ÊñáÂ≠ó {n_text} ‰∏™Â≠óÊÆµ(>={self.cfg['text_fields_threshold']}) = {s} ÂàÜ")
            elif n_text > 0:
                details.append(f"ÂèØËØÜÂà´ÊñáÂ≠ó {n_text} ‰∏™Â≠óÊÆµ(<{self.cfg['text_fields_threshold']}) = 0 ÂàÜ")

            meets = score > self.cfg["score_threshold"]
            if meets:
                category = "qualified"; reason = f'ÊÄªÂàÜ {score} ÂàÜ > {self.cfg["score_threshold"]} ÂàÜÔºåÁ¨¶ÂêàË¶ÅÊ±Ç'
            else:
                if score == 0:
                    category = "no_content"; reason = f'ÊÄªÂàÜ {score} ÂàÜÔºåÊó†‰ªª‰ΩïÊúâÊïàÂÜÖÂÆπ'
                else:
                    category = "insufficient_score"; reason = f'ÊÄªÂàÜ {score} ÂàÜ ‚â§ {self.cfg["score_threshold"]} ÂàÜÔºå‰∏çÁ¨¶ÂêàË¶ÅÊ±Ç'

            analysis = {
                "filename": fname,
                "frontal_faces": n_face,
                "license_plates": n_plate,
                "text_count": n_text,
                "total_score": score,
                "score_details": details,
                "meets_requirements": meets,
                "score_threshold": self.cfg["score_threshold"],
                "face_details": face_det,
                "plate_details": plate_det,
                "text_details": text_det,
                "category": category,
                "reason": reason,
                "timestamp": time.time()
            }
            return category, analysis
        except Exception as e:
            return "failed", {"filename": os.path.basename(image_path), "error": str(e)}

    def _move_to(self, image_path: str, category: str) -> bool:
        target_dir = {"qualified": self.qual_dir, "insufficient_score": self.insuf_dir, "no_content": self.empty_dir}.get(category)
        if target_dir is None:
            return False
        name = os.path.basename(image_path)
        dst = target_dir / name
        c = 1
        while dst.exists():
            stem, ext = os.path.splitext(name)
            dst = target_dir / f"{stem}_{c}{ext}"
            c += 1
        try:
            shutil.move(image_path, dst)
            return True
        except Exception as e:
            self.logger.error(f"‚ùå ÁßªÂä®Â§±Ë¥• {image_path} -> {dst}: {e}")
            return False

    def _image_files(self) -> List[str]:
        p = Path(self.cfg["input_dir"])
        if not p.exists():
            self.logger.error(f"‚ùå ËæìÂÖ•ÁõÆÂΩï‰∏çÂ≠òÂú®: {p}")
            return []
        files = []
        for f in sorted(p.iterdir()):
            if f.is_file() and f.suffix.lower() in self.SUPPORTED_FORMATS:
                files.append(str(f))
        self.logger.info(f"üìä ÊâæÂà∞ {len(files)} ‰∏™ÂõæÂÉèÊñá‰ª∂")
        return files

    def _save_results(self):
        analysis_file = self.analysis_dir / "classification_analysis.json"
        with open(analysis_file, "w", encoding="utf-8") as f:
            json.dump(self.analysis_results, f, indent=2, ensure_ascii=False)

        summary_file = self.analysis_dir / "classification_summary.json"
        total = len(self.analysis_results)
        summary = {
            "total_processed": total,
            "statistics": self.stats.copy(),
            "processing_time": time.time() - self.start_time,
            "scoring_system": {
                "clear_face_score": self.cfg["clear_face_score"],
                "clear_plate_score": self.cfg["clear_plate_score"],
                "text_recognition_score": self.cfg["text_score"],
                "text_fields_threshold": self.cfg["text_fields_threshold"],
                "score_threshold": self.cfg["score_threshold"]
            },
            "configuration": {
                "yaw_angle_threshold": self.cfg["yaw_thresh"],
                "min_face_confidence": self.cfg["min_face_conf"],
                "min_plate_confidence": self.cfg["min_plate_conf"],
                "min_text_confidence": self.cfg["min_text_conf"]
            },
            "timestamp": time.time()
        }
        with open(summary_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        self.logger.info(f"üìä ÂàÜÊûêÁªìÊûú: {analysis_file}")
        self.logger.info(f"üìä ÁªüËÆ°ÊëòË¶Å: {summary_file}")

    def _print_final(self):
        dt = time.time() - self.start_time
        total = sum(self.stats.values())
        self.logger.info("="*80)
        self.logger.info("üéâ ÂàÜÁ±ªÂÆåÊàêÔºàÊñ∞ËÆ°ÂàÜÁ≥ªÁªüÔºâ")
        self.logger.info(f"‚úÖ ÂêàÊ†º: {self.stats['qualified']} | ‚ùå ‰∏çË∂≥: {self.stats['insufficient_score']} | ‚≠ï Êó†ÂÜÖÂÆπ: {self.stats['no_content']} | üí• Â§±Ë¥•: {self.stats['failed']}")
        self.logger.info(f"üìä ÊÄªÂ§ÑÁêÜ: {total}  | ‚è∞ Áî®Êó∂: {dt:.1f}s")
        if total:
            self.logger.info(f"üöÄ ÈÄüÂ∫¶: {total/dt:.1f} Âº†/Áßí")
        for name, d in [("Á¨¶ÂêàÊù°‰ª∂", self.qual_dir), ("ÂàÜÊï∞‰∏çÂ§ü", self.insuf_dir), ("Êó†‰ªª‰ΩïÂÜÖÂÆπ", self.empty_dir)]:
            if d.exists():
                c = len([f for f in d.iterdir() if f.is_file() and f.suffix.lower() in self.SUPPORTED_FORMATS])
                self.logger.info(f"  üìÅ {name}: {c}")
        self.logger.info("="*80)

    def run(self):
        from tqdm import tqdm
        import torch

        self.logger.info("üöÄ ÂêØÂä®ÂàÜÁ±ªÂô®‚Ä¶")
        self.logger.info(f"üìÅ ËæìÂÖ•: {self.cfg['input_dir']} / ËæìÂá∫Âü∫ÂáÜ: {self.cfg['output_base']} / ËÆæÂ§á: {self.device}")

        files = self._image_files()
        if not files:
            self.logger.warning("‚ùå Êú™ÊâæÂà∞ÂõæÂÉè"); return

        with tqdm(total=len(files), desc="ÂàÜÁ±ªËøõÂ∫¶", ncols=120,
                  bar_format='{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]') as pbar:
            for i, imgp in enumerate(files):
                cat, ana = self.classify_image(imgp)
                if cat != "failed":
                    if self._move_to(imgp, cat):
                        self.stats[cat] += 1
                    else:
                        self.stats["failed"] += 1
                        ana["move_failed"] = True
                else:
                    self.stats["failed"] += 1
                self.analysis_results.append(ana)
                pbar.update(1)

                if i % self.cfg["progress_update_frequency"] == 0 and i > 0:
                    s = f"‚úÖ{self.stats['qualified']} ‚ùå{self.stats['insufficient_score']} ‚≠ï{self.stats['no_content']}"
                    pbar.set_description(f"ÂàÜÁ±ªËøõÂ∫¶ ({s})")
                if i % self.cfg["gc_frequency"] == 0 and i > 0 and 'cuda' in self.device:
                    torch.cuda.empty_cache(); gc.collect()

        self._save_results()
        self._print_final()

# ========= ‰∫§‰∫íÂºèÂÖ•Âè£ =========
def main():
    print("===== üé¨ ‰∫§‰∫íÂºè‰∏ãËΩΩ‚ÜíÊäΩÂ∏ß‚ÜíÂàÜÁ±ª Â∑•ÂÖ∑ =====")
    url = input("ËØ∑ËæìÂÖ•ËßÜÈ¢ëÈìæÊé•ÊàñÊú¨Âú∞Êñá‰ª∂Ë∑ØÂæÑ: ").strip()
    if not url:
        print("‚ùå ÂøÖÈ°ªËæìÂÖ•ÈìæÊé•ÊàñÊú¨Âú∞Ë∑ØÂæÑ"); sys.exit(1)

    video_number = input("ËØ∑ËæìÂÖ•ËßÜÈ¢ëÁºñÂè∑ÔºàÁî®‰∫éÂëΩÂêçÔºåÂ¶Ç 38Ôºâ: ").strip()
    if not video_number or not re.fullmatch(r"\d+", video_number):
        print("‚ùå ËßÜÈ¢ëÁºñÂè∑ÂøÖÈ°ªÊòØÊï∞Â≠ó"); sys.exit(1)

    fps_in = input("ÊØèÁßíÊäΩÂ∏ßÊï∞ÔºàÈªòËÆ§=3ÔºåÁõ¥Êé•ÂõûËΩ¶‰ΩøÁî®ÈªòËÆ§Ôºâ: ").strip()
    fps = float(fps_in) if fps_in else 3.0

    jpg_in = input("JPGË¥®ÈáèÔºàÈªòËÆ§=1ÔºåË∂äÂ∞èË∂äÊ∏ÖÊô∞Ôºå2-31ÔºõÁõ¥Êé•ÂõûËΩ¶=1Ôºâ: ").strip()
    jpg_q = int(jpg_in) if jpg_in else 1

    # Áî®‰Ω†ÁöÑË∑ØÂæÑÂ∏ÉÂ±Ä
    base_dir = Path("/home/zhiqics/sanjian/predata")
    video_dir = base_dir / "videos"
    frames_dir = base_dir / f"output_frames{video_number}"

    # 1) Ëé∑ÂèñËßÜÈ¢ë
    downloaded = False
    if url.startswith("http"):
        video_path = download_video(url, video_dir, video_number)
        downloaded = True
    else:
        video_path = Path(url)
        if not video_path.exists():
            print(f"‚ùå Êú¨Âú∞ËßÜÈ¢ë‰∏çÂ≠òÂú®Ôºö{video_path}"); sys.exit(1)
        ensure_ffmpeg_compatible(video_path)

    # 2) ÊäΩÂ∏ß
    ok = extract_frames_ffmpeg(
        video_path=video_path,
        output_dir=frames_dir,
        fps=fps,
        jpg_q=jpg_q,
        video_number=video_number
    )
    if not ok:
        sys.exit(1)

    # 3) ÊàêÂäüÂêéÂà†Èô§‚ÄúÊú¨Ê¨°‰∏ãËΩΩ‚ÄùÁöÑËßÜÈ¢ëÊñá‰ª∂
    if downloaded:
        try:
            video_path.unlink(missing_ok=True)
            print(f"üßπ Â∑≤Âà†Èô§‰∏ãËΩΩËßÜÈ¢ëÔºö{video_path}")
            try:
                if video_dir.exists() and not any(video_dir.iterdir()):
                    video_dir.rmdir()
                    print(f"üßº Â∑≤Ê∏ÖÁêÜÁ©∫ÁõÆÂΩïÔºö{video_dir}")
            except Exception:
                pass
        except Exception as e:
            print(f"‚ö†Ô∏è Âà†Èô§‰∏ãËΩΩËßÜÈ¢ëÂ§±Ë¥•Ôºö{e}")

    # 4) ÁªßÁª≠ÊâßË°åÔºöÊ≠£ËÑ∏/ËΩ¶Áâå/OCR ÂàÜÁ±ª
    print("\n===== üß† ÂºÄÂßãÂõæÂÉèÂàÜÁ±ªÔºàÊ≠£ËÑ∏/ËΩ¶Áâå/OCRÔºâ =====")
    default_model = str(base_dir / "models/license_plate_detector.pt")
    model_path_in = input(f"ËØ∑ËæìÂÖ•ËΩ¶ÁâåÊ£ÄÊµãÊ®°ÂûãË∑ØÂæÑÔºàÂõûËΩ¶ÈªòËÆ§: {default_model}Ôºâ: ").strip()
    plate_model = model_path_in if model_path_in else default_model

    use_gpu_in = input("ÊòØÂê¶ÂêØÁî®GPU? (Y/nÔºåÈªòËÆ§Y): ").strip().lower()
    use_gpu = False if use_gpu_in == "n" else True
    gpu_id_in = input("GPUËÆæÂ§áIDÔºàÈªòËÆ§=0Ôºâ: ").strip()
    gpu_id = int(gpu_id_in) if gpu_id_in else 0

    # ‰æùËµñÊ£ÄÊü•
    if not check_py_deps():
        sys.exit(1)

    # ‰ºòÂÖàÂ∞ùËØïÂä®ÊÄÅÂä†ËΩΩ face_plate_classifier.py
    classifier_py = Path(__file__).parent / "face_plate_classifier.py"
    try:
        if classifier_py.exists():
            print(f"üß© Âä®ÊÄÅÂä†ËΩΩÂàÜÁ±ªÂô®Ê®°Âùó: {classifier_py}")
            mod = load_classifier_module(classifier_py)
            run_classifier(mod, frames_dir, Path(plate_model))
        else:
            print("‚ö†Ô∏è Êú™ÊâæÂà∞ face_plate_classifier.pyÔºå‰ΩøÁî®ÂÜÖÁΩÆ FacePlateClassifier Á±ª")
            clf = FacePlateClassifier(
                input_dir=str(frames_dir),
                output_base=str(frames_dir),
                plate_model_path=plate_model,
                score_threshold=5,
                clear_face_score=2,
                clear_plate_score=2,
                text_score=2,
                text_fields_threshold=10,
                yaw_thresh=35.0,
                min_face_conf=0.8,
                min_plate_conf=0.5,
                min_face_size=60,
                min_plate_size=50,
                min_face_clarity=30.0,
                face_area_px2=3600,
                face_dist_ratio=0.3,
                min_text_conf=0.5,
                min_text_len=3,
                use_gpu=use_gpu,
                gpu_id=gpu_id,
                enable_torch_opt=True,
                gc_frequency=100,
                progress_update_frequency=50
            )
            clf.run()
    except KeyboardInterrupt:
        print("‚ö° Áî®Êà∑‰∏≠Êñ≠")
    except Exception as e:
        print(f"‚ùå ÂàÜÁ±ªÊµÅÁ®ãÂºÇÂ∏∏Ôºö{e}")
        import traceback; print(traceback.format_exc())

if __name__ == "__main__":
    main()
